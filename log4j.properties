# Set the root logger level to INFO and its appender to the console
log4j.rootCategory=WARN, console

# Define the console appender
log4j.appender.console=org.apache.log4j.ConsoleAppender
log4j.appender.console.target=System.err
log4j.appender.console.layout=org.apache.log4j.PatternLayout
log4j.appender.console.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{1}: %m%n

# Till above, these two sections: category and appender will set the root level log4j, and they will stop
# all the log messages sent by the spark and other packages except warning and errors.
# So, we will get clean and minimal log output
# -------------------------------------------------------------------------

# But, we want to change the log settings for our application,
# Now, I will define second log level for my pyspark application(code)

# My application log
log4j.logger.ashish.spark.example= INFO, console, file
log4j.additivity.ashish.spark.example= false

# define rolling file appender
# A Rolling File Appender in Log4j is a type of file appender that writes log messages to a file,
# but with the ability to roll (rotate) logs when certain conditions are met, such as:
#   - File size limit is reached (e.g., 500MB).
#   - Backup file count is exceeded (e.g., keep the last 2 logs).

log4j.appender.file=org.apache.log4j.RollingFileAppender
log4j.appender.file.File=${spark.yarn.app.container.log.dir}/${logfile.name}.log

# at line 29, we need to notice about these 2 variables: ${spark.yarn.app.container.log.dir}/${logfile.name}.log 
# the first variable is to identify the log file directory location
# the second variable is to identify the log file name.
# as Spark run its code as Master-slave architure, so our application will run as driver and multiple executors, so that is why we kept the variable names which is coming from cluster manager to manage the logs.


# define following in Java System
# -Dlog4j.configuration=file:log4j.properties
# -Dlogfile.name=hello-spark
# -Dspark.yarn.app.container.log.dir=app-logs

log4j.appender.file.ImmediateFlush=true
log4j.appender.file.Append=false
log4j.appender.file.MaxFileSize=500MB
log4j.appender.file.MaxBackupIndex=2
log4j.appender.file.layout=org.apache.log4j.PatternLayout
log4j.appender.file.layout.conversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{1}: %m%n

# ----------------------------------------------------------------------
# Recommendations from Spark template: These are recommended by Spark
# This configuration sets logging levels for various Spark and Hadoop components, 
# reducing noise from unnecessary logs while ensuring important information is logged.
log4j.logger.org.apache.spark.repl.Main=WARN
log4j.logger.org.spark_project.jetty=WARN
log4j.logger.org.spark_project.jetty.util.component.AbstractLifeCycle=ERROR
log4j.logger.org.apache.spark.repl.SparkIMain$exprTyper=INFO
log4j.logger.org.apache.spark.repl.SparkILoop$SparkILoopInterpreter=INFO
log4j.logger.org.apache.parquet=ERROR
log4j.logger.org.apache.hadoop.hive.ql.exec.FunctionRegistry=ERROR
