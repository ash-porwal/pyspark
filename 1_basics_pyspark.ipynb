{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When running spark code in pypsark-shell or in Databricks or other cloud environment, they gives us spark session ready to use  \n",
    "Means we can work on data processing just by start writing `spark.<methods>`.  \n",
    "\n",
    "But when we build our application which we need to submit to spark cluster, then we need to import `SparkSession` `from pyspark.sql`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logging in PySpark with Log4j\n",
    "\n",
    "For logging purposes in PySpark, we need to work with `log4j`, but we don't need to install it separately, as `PySpark` comes with `log4j` by default.\n",
    "\n",
    "## What is Log4j?  \n",
    "Log4j is similar to Python's logging module. It has three main components:\n",
    "\n",
    "- **Logger**: Captures logging information.\n",
    "- **Configuration**: Defines how logging information is captured and formatted in `log4j.properties` file.\n",
    "- **Appender**: They are output destinations like where to write these logs (console, file, etc.). These appenders also configured in the `log4j.properties` file.\n",
    "\n",
    "The most important thing in log4j is **Configuration**. And configuration is defined in `log4j.properties` file, so we need to understand this file.\n",
    "- Log4j configuration file which is: `log4j.properties` file, so there we define configuration in hierarchy.\n",
    "    - The Topmost hierarchy is the root category in the file, which is this line: `log4j.rootCategory=INFO, console`.  \n",
    "        For any category, we define 2 things: \n",
    "        - First thing is log level: `INFO` `WARN` `DEBUG` `ERROR` So, these are  log levels log4j supports\n",
    "        - Second thing is a list of appender: `console`: So, when we set log levels like `INFO` or `WARN` then I set the appender as `console`, so I want these log messages go to console.\n",
    "- Next thing in the log config file, we need to define console appender:  \n",
    "    - console appender settings looks like this:\n",
    "        ```\n",
    "        # Define the console appender\n",
    "        log4j.appender.console=org.apache.log4j.ConsoleAppender\n",
    "        log4j.appender.console.target=System.err\n",
    "        log4j.appender.console.layout=org.apache.log4j.PatternLayout\n",
    "        log4j.appender.console.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{1}: %m%n\n",
    "    And above appender setting is standard settings, they remain almost the same in the most pyspark projects.\n",
    "\n",
    "- Till above, these two sections: **category** and **appender** will set the root level log4j,  \n",
    "    and they will stop all the log messages sent by the spark and other packages except warning and errors.  \n",
    "    So, we will get clean and minimal log output.\n",
    "\n",
    "- But, we want to change the log settings for our application(pyspark code),  \n",
    "    Now, I will define second log level for my pyspark application(code)\n",
    "    for my application, I will name level as: `log4j.additivity.ashish.spark.example`, So, this is the name that I am going to use when using the Logger in my application.\n",
    "      \n",
    "    `log4j.logger.ashish.spark.example= INFO, console, file` #here, I've set my application log level to INFO and set log should go to **console** and **log file**\n",
    "\n",
    "## How to Use Log4j?\n",
    "\n",
    "### 1. Create a Custom `log4j.properties` File\n",
    "\n",
    "To customize logging behavior, create a `log4j.properties` file (or any preferred name) with the following configuration.\n",
    "\n",
    "#### Example Configuration (`log4j.properties`):\n",
    "```properties\n",
    "# Set the root logger level to INFO and its appender to the console\n",
    "log4j.rootCategory=INFO, console\n",
    "\n",
    "# Define the console appender\n",
    "log4j.appender.console=org.apache.log4j.ConsoleAppender\n",
    "log4j.appender.console.target=System.err\n",
    "log4j.appender.console.layout=org.apache.log4j.PatternLayout\n",
    "log4j.appender.console.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{1}: %m%n\n",
    "\n",
    "# Optional: Set a specific logger level for Spark components\n",
    "log4j.logger.org.apache.spark=INFO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is how we start SparkSession with `builder` attribute.\n",
    "- Each spark session can only have 1 active spark session because spark session is a driver we cannot have more than 1 driver in our spark application.\n",
    "- `.builder` is attribute of SparkSession class, builder helps in creating and configuring SparkSession."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark = SparkSession.builder \\\n",
    "#     .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BTW, spark is a very highly configurable system, so SparkSession is also highly configurable, so we can have some config according to ourself in our sparksession, so instead of above, we can set some config when creating `SparkSession`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add some config in SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/20 17:30:34 WARN Utils: Your hostname, lenovo resolves to a loopback address: 127.0.1.1; using 192.168.29.125 instead (on interface wlp3s0)\n",
      "25/02/20 17:30:34 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/02/20 17:30:35 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/02/20 17:30:37 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "        .appName(\"1_basics_pyspark\") \\\n",
    "        .master(\"local[3]\") \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, in above code is like this:\n",
    "```python\n",
    "spark = SparkSession.builder \\\n",
    "        .appName(\"1_basics_pyspark\")  # This is a our Spark Application name, think this as a log file name in logging module, so if my Spark application I've build like to fetch GCB-Billing data, so I would set as: .appName(\"GCPBILLING\")\n",
    "        .master(\"local[3]\")  # here we are setting Cluster Manager, and Cluster Manager config is defined as Spark Master.\n",
    "        .getOrCreate()\n",
    "```\n",
    "As, my code is running on local environment, so I set master to `local[3]` **which means, I am using local multithreaded JVM with 3 threads**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And, Above is how we created spark session, that means we created our spark driver, now we can use this driver for data processing.\n",
    "And **once we are done with data processing, we should stop the driver**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stopping the driver\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
